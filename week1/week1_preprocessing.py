# -*- coding: utf-8 -*-
"""Week1 - Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eUZuKu2bAXpfBn8pVWXoDxEfDVLCn84w
"""

# Install libraries
#!pip install pandas spacy nltk datasets

# Download spaCy English model
#!python -m spacy download en_core_web_sm

import pandas as pd
import re
import spacy
from datasets import load_dataset


def clean_text(text):
  # this functions cleans the text
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"@\w+", "", text)
  text = re.sub(r"#\w+", "", text)
  text = text.lower()
  return text

def spacy_preprocess(text):
  doc = nlp(text)
  return " ".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])

if __name__ == '__main__':

    nlp = spacy.load("en_core_web_sm")

    dataset = load_dataset("tweet_eval", "hate")
    df = pd.DataFrame(dataset["train"])
    print(df.head())


    # Tokenization and lemmatization using SPACY
    print(spacy_preprocess(clean_text("Cats are runnig faster than the dogs!")))

    df["clean_text"] = df["text"].apply(lambda x: spacy_preprocess(clean_text(x)))
    print(df.head())

    # save to csv data cleaned:
    df.to_csv("data_cleaned.csv", index=False)

    # or for download it from colab:
    #from google.colab import files
    #files.download("data_cleaned.csv")